# dAImond üíé

Deep Learnin —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è Ruby, –≤–¥–æ—Ö–Ω–æ–≤–ª—ë–Ω–Ω—ã–π PyTorch.

[![Gem Version](https://badge.fury.io/rb/daimond.svg)](https://rubygems.org/gems/daimond)
[![Ruby](https://img.shields.io/badge/ruby-%23CC342D.svg?style=for-the-badge&logo=ruby&logoColor=white)](https://www.ruby-lang.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

> **–ü–æ—á–µ–º—É Ruby?** –•–ó, –∑–∞—Ö–æ—Ç–µ–ª–æ—Å—å. dAImond –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–¥–æ—Å—Ç—å –≤ –≤–æ–∑—é–∫–∞–Ω–∏–∏ —Å ML, –ø–æ—Ç–æ–º—É —á—Ç–æ —ç—Ç–æ Ruby. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π Rust backend –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤ 50-100 —Ä–∞–∑.

## –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- üî• **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ** - –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π autograd —Å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –≥—Ä–∞—Ñ–∞–º–∏
- üß† **–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏** - –õ–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏, –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (ReLU, Sigmoid, Softmax, Tanh)
- üìä **–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã** - SGD —Å –º–æ–º–µ–Ω—Ç—É–º–æ–º, –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ learning rate
- üéØ **–§—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å** - MSE, CrossEntropy
- üíæ **–°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π** - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ/–∑–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ Marshal
- üìà **–ó–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö** - Batch processing, —à–∞—Ñ—Ñ–ª, –ø–æ–¥–¥–µ—Ä–∂–∫–∞ MNIST
- ‚ö° **–ë—ã—Å—Ç—Ä—ã–π –±—ç–∫–µ–Ω–¥** - Numo::NArray –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π (—Å–∫–æ—Ä–æ—Å—Ç—å C)
- üé® **–ö—Ä–∞—Å–∏–≤—ã–π API** - –ò–¥–∏–æ–º–∞—Ç–∏—á–Ω—ã–π Ruby DSL, —á–µ–π–Ω—è—â–∏–µ—Å—è –º–µ—Ç–æ–¥—ã

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞

–î–æ–±–∞–≤—å—Ç–µ –≤ Gemfile:

```ruby
gem 'daimond'
```


–ò–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Ä—É—á–∫–∞–º–∏:
```ruby
gem install daimond
```

**–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:** Ruby 2.7+, numo-narray

## –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç
```ruby
require 'daimond'

# Define your model
class NeuralNet < Daimond::NN::Module
  attr_reader :fc1, :fc2
  
  def initialize
    super()
    @fc1 = Daimond::NN::Linear.new(784, 128)
    @fc2 = Daimond::NN::Linear.new(128, 10)
    @parameters = @fc1.parameters + @fc2.parameters
  end
  
  def forward(x)
    x = @fc1.forward(x).relu
    @fc2.forward(x).softmax
  end
end

# Training loop
model = NeuralNet.new
optimizer = Daimond::Optim::SGD.new(model.parameters, lr: 0.1, momentum: 0.9)
criterion = Daimond::Loss::CrossEntropyLoss.new

# Forward ‚Üí Backward ‚Üí Update
loss = criterion.call(model.forward(input), target)
optimizer.zero_grad
loss.backward!
optimizer.step
```

## –ü—Ä–∏–º–µ—Ä MNIST (97% Accuracy!)
**–û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞ 60–∫ —Ä—É–∫–æ–ø–∏—Å–Ω—ã—Ö —Ü–∏—Ñ—Ä–∞—Ö:**
```ruby
ruby examples/mnist.rb
```
**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:**
```text
Epoch 1/5: Loss = 0.2898, Accuracy = 91.35%
Epoch 2/5: Loss = 0.1638, Accuracy = 95.31%
Epoch 3/5: Loss = 0.1389, Accuracy = 96.2%
Epoch 4/5: Loss = 0.1195, Accuracy = 96.68%
Epoch 5/5: Loss = 0.1083, Accuracy = 97.12%
```

**–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏:**
```ruby
model.save('models/mnist_model.bin')
```

**–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–∏–∫—Ç:**
```ruby
model = NeuralNet.new
model.load('models/mnist_model.bin')
prediction = model.forward(test_image)
```

## –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
| Backend   | MNIST (60k) Speed | Accuracy   |
| --------- | ----------------- | ---------- |
| Pure Ruby | ~30 min/epoch     | 97%        |
| Numo (C)  | ~3 min/epoch      | 97%        |
| **Rust**  | **~12 sec/epoch** | **89-98%** |

## –ë—ç–∫–µ–Ω–¥ –Ω–∞ —Ä–∞—Å—Ç–µ
For 50-100x speedup, compile Rust extensions:
```bash
cd ext/daimond_rust
cargo build --release
cd ../..
ruby examples/mnist_conv_rust.rb
```

## Roadmap
- [x] –Ø–¥—Ä–æ Autograd
- [x] –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ –∏ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏
- [x] MNIST 97%  (Adam)
- [x] Rust backend
- [x] –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
- [ ] Batch Normalization & Dropout
- [ ] OpenCL/CUDA via FFI
- [ ] ONNX

## –ü–æ–º–æ—â—å
–ë—É–¥—É —Ä–∞–¥ –ª—é–±–æ–π –ø–æ–º–æ—â–∏! –ò–Ω—Ñ–∞ –≤ CONTRIBUTING.md.

## –õ–∏—Ü–µ–Ω–∑–∏—è
MIT License