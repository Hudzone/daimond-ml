# dAImond üíé

Deep Learnin —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è Ruby, –≤–¥–æ—Ö–Ω–æ–≤–ª—ë–Ω–Ω—ã–π PyTorch.

[![Ruby](https://img.shields.io/badge/ruby-%23CC342D.svg?style=for-the-badge&logo=ruby&logoColor=white)](https://www.ruby-lang.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

> **–ü–æ—á–µ–º—É Ruby?** –•–ó, –∑–∞—Ö–æ—Ç–µ–ª–æ—Å—å. dAImond –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–¥–æ—Å—Ç—å –≤ –≤–æ–∑—é–∫–∞–Ω–∏–∏ —Å ML, –ø–æ—Ç–æ–º—É —á—Ç–æ —ç—Ç–æ Ruby.

## –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- üî• **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ** - –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π autograd —Å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –≥—Ä–∞—Ñ–∞–º–∏
- üß† **–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏** - –õ–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏, –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (ReLU, Sigmoid, Softmax, Tanh)
- üìä **–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã** - SGD —Å –º–æ–º–µ–Ω—Ç—É–º–æ–º, –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ learning rate
- üéØ **–§—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å** - MSE, CrossEntropy
- üíæ **–°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π** - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ/–∑–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ Marshal
- üìà **–ó–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö** - Batch processing, —à–∞—Ñ—Ñ–ª, –ø–æ–¥–¥–µ—Ä–∂–∫–∞ MNIST
- ‚ö° **–ë—ã—Å—Ç—Ä—ã–π –±—ç–∫–µ–Ω–¥** - Numo::NArray –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π (—Å–∫–æ—Ä–æ—Å—Ç—å C)
- üé® **–ö—Ä–∞—Å–∏–≤—ã–π API** - –ò–¥–∏–æ–º–∞—Ç–∏—á–Ω—ã–π Ruby DSL, —á–µ–π–Ω—è—â–∏–µ—Å—è –º–µ—Ç–æ–¥—ã

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞

–î–æ–±–∞–≤—å—Ç–µ –≤ Gemfile:

```ruby
gem 'daimond'
```


–ò–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Ä—É—á–∫–∞–º–∏:
```ruby
gem install daimond
```

**–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:** Ruby 2.7+, numo-narray

## –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç
```ruby
require 'daimond'

# Define your model
class NeuralNet < Daimond::NN::Module
  attr_reader :fc1, :fc2
  
  def initialize
    super()
    @fc1 = Daimond::NN::Linear.new(784, 128)
    @fc2 = Daimond::NN::Linear.new(128, 10)
    @parameters = @fc1.parameters + @fc2.parameters
  end
  
  def forward(x)
    x = @fc1.forward(x).relu
    @fc2.forward(x).softmax
  end
end

# Training loop
model = NeuralNet.new
optimizer = Daimond::Optim::SGD.new(model.parameters, lr: 0.1, momentum: 0.9)
criterion = Daimond::Loss::CrossEntropyLoss.new

# Forward ‚Üí Backward ‚Üí Update
loss = criterion.call(model.forward(input), target)
optimizer.zero_grad
loss.backward!
optimizer.step
```

## –ü—Ä–∏–º–µ—Ä MNIST (97% Accuracy!)
**–û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞ 60–∫ —Ä—É–∫–æ–ø–∏—Å–Ω—ã—Ö —Ü–∏—Ñ—Ä–∞—Ö:**
```ruby
ruby examples/mnist.rb
```
**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:**
```text
Epoch 1/5: Loss = 0.2898, Accuracy = 91.35%
Epoch 2/5: Loss = 0.1638, Accuracy = 95.31%
Epoch 3/5: Loss = 0.1389, Accuracy = 96.2%
Epoch 4/5: Loss = 0.1195, Accuracy = 96.68%
Epoch 5/5: Loss = 0.1083, Accuracy = 97.12%
```

**–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏:**
```ruby
model.save('models/mnist_model.bin')
```

**–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–∏–∫—Ç:**
```ruby
model = NeuralNet.new
model.load('models/mnist_model.bin')
prediction = model.forward(test_image)
```

## –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
–•–æ—Ç—è —á–∏—Å—Ç—ã–π Ruby –º–µ–¥–ª–µ–Ω–Ω–µ–µ PyTorch/CUDA, dAImond –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Ä–∞–∑—É–º–Ω—É—é —Å–∫–æ—Ä–æ—Å—Ç—å –¥–ª—è –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –Ω–µ–±–æ–ª—å—à–∏—Ö/—Å—Ä–µ–¥–Ω–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤:
MNIST (60k –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π): ~2-3 –º–∏–Ω—É—Ç—ã –Ω–∞ —ç–ø–æ—Ö—É –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º CPU
–ò–¥–µ–∞–ª—å–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∏ –º–æ–¥–µ–ª–µ–π < 1M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

## –ü–ª–∞–Ω—ã
- [x] –Ø–¥—Ä–æ autograd
- [x] –õ–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
- [x] MNIST 97% —Ç–æ—á–Ω–æ—Å—Ç–∏
- [x] –°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
- [ ] –°–≤—ë—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ (Conv2D)
- [ ] Batch Normalization –∏ Dropout
- [ ] –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã Adam/RMSprop
- [ ] –ü–æ–¥–¥–µ—Ä–∂–∫–∞ GPU (OpenCL/CUDA via FFI)
- [ ] ONNX —ç–∫—Å–ø–æ—Ä—Ç/–∏–º–ø–æ—Ä—Ç

## –ü–æ–º–æ—â—å
–ë—É–¥—É —Ä–∞–¥ –ª—é–±–æ–π –ø–æ–º–æ—â–∏! –ò–Ω—Ñ–∞ –≤ CONTRIBUTING.md.

## –õ–∏—Ü–µ–Ω–∑–∏—è
MIT License